{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import conv\n",
    "from torch.nn.modules.utils import _pair\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.nn.functional as F\n",
    "from torchvision  import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SpectralNorm(nn.Module):\\n    def __init__(self, module, name=\\'weight\\', power_iterations=1):\\n        super(SpectralNorm, self).__init__()\\n        self.module = module\\n        self.name = name\\n        self.power_iterations = power_iterations\\n        if not self._made_params():\\n            self._make_params()\\n\\n    def _update_u_v(self):\\n        u = getattr(self.module, self.name + \"_u\")\\n        v = getattr(self.module, self.name + \"_v\")\\n        w = getattr(self.module, self.name + \"_bar\")\\n\\n        height = w.data.shape[0]\\n        for _ in range(self.power_iterations):\\n            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\\n            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\\n\\n        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\\n        sigma = u.dot(w.view(height, -1).mv(v))\\n        setattr(self.module, self.name, w / sigma.expand_as(w))\\n\\n    def _made_params(self):\\n        try:\\n            u = getattr(self.module, self.name + \"_u\")\\n            v = getattr(self.module, self.name + \"_v\")\\n            w = getattr(self.module, self.name + \"_bar\")\\n            return True\\n        except AttributeError:\\n            return False\\n\\n\\n    def _make_params(self):\\n        w = getattr(self.module, self.name)\\n\\n        height = w.data.shape[0]\\n        width = w.view(height, -1).data.shape[1]\\n\\n        u = nn.Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\\n        v = nn.Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\\n        u.data = l2normalize(u.data)\\n        v.data = l2normalize(v.data)\\n        w_bar = nn.Parameter(w.data)\\n\\n        del self.module._parameters[self.name]\\n\\n        self.module.register_parameter(self.name + \"_u\", u)\\n        self.module.register_parameter(self.name + \"_v\", v)\\n        self.module.register_parameter(self.name + \"_bar\", w_bar)\\n\\n\\n    def forward(self, *args):\\n        self._update_u_v()\\n        return self.module.forward(*args)\\n\\n\\nclass SNConv2d(SpectralNorm):\\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\\n        module = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\\n        super(SNConv2d, self).__init__(module)'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "def max_singular_value(W, u=None, Ip=1):\n",
    "    \"\"\"\n",
    "    power iteration for weight parameter\n",
    "    \"\"\"\n",
    "    if u is None:\n",
    "        u = torch.FloatTensor(1, W.size(0)).normal_()\n",
    "        \n",
    "    _u = u\n",
    "    for _ in range(Ip):\n",
    "        _v = l2normalize(torch.matmul(_u, W), eps=1e-12)\n",
    "        _u = l2normalize(torch.matmul(_v, torch.transpose(W, 0, 1)), eps=1e-12)\n",
    "        \n",
    "    sigma = torch.sum(F.linear(_u, torch.transpose(W, 0, 1)) * _v)\n",
    "    return sigma, _u\n",
    "    \n",
    "    \n",
    "class SNConv2d(conv._ConvNd):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(SNConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, False, _pair(0), groups, bias)\n",
    "        self.u = nn.Parameter(torch.Tensor(1, out_channels).normal_(), requires_grad=False)\n",
    "        #self.u = torch.Tensor(1, out_channels).normal_()\n",
    "\n",
    "    @property\n",
    "    def W_(self):\n",
    "        w_mat = self.weight.view(self.weight.size(0), -1).data\n",
    "        sigma, _u = max_singular_value(w_mat, self.u.data)\n",
    "        self.u.data = _u\n",
    "        return self.weight / sigma\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.conv2d(input, self.W_, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\"\"\"class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = nn.Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = nn.Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = nn.Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "\n",
    "class SNConv2d(SpectralNorm):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        module = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
    "        super(SNConv2d, self).__init__(module)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, zdim=100, n_feature_maps=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            #1x1\n",
    "            nn.ConvTranspose2d(zdim, 8*n_feature_maps, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(8*n_feature_maps),\n",
    "            nn.ReLU(True),\n",
    "            #4x4\n",
    "            nn.ConvTranspose2d(8*n_feature_maps, 4*n_feature_maps, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(4*n_feature_maps),\n",
    "            nn.ReLU(True),\n",
    "            #8x8\n",
    "            nn.ConvTranspose2d(4*n_feature_maps, 2*n_feature_maps, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(2*n_feature_maps),\n",
    "            nn.ReLU(True),\n",
    "            #16x16\n",
    "            nn.ConvTranspose2d(2*n_feature_maps, n_feature_maps, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(n_feature_maps),\n",
    "            nn.ReLU(True),\n",
    "            #32x32\n",
    "            nn.ConvTranspose2d(n_feature_maps, 3, 4, 2, 1, bias=False),\n",
    "            #64x64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def  forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class SNDiscriminator(nn.Module):\n",
    "    def __init__(self, n_feature_maps=64):\n",
    "        super(SNDiscriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            #64x64\n",
    "            #SpectralNorm(nn.Conv2d(3, n_feature_maps, 4, 2, 1, bias=False)),\n",
    "            SNConv2d(3, n_feature_maps, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #32x32\n",
    "            #SpectralNorm(nn.Conv2d(n_feature_maps, 2*n_feature_maps, 4, 2, 1, bias=False)),\n",
    "            SNConv2d(n_feature_maps, 2*n_feature_maps, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #16x16\n",
    "            #SpectralNorm(nn.Conv2d(2*n_feature_maps, 4*n_feature_maps, 4, 2, 1, bias=False)),\n",
    "            SNConv2d(2*n_feature_maps, 4*n_feature_maps, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #8x8\n",
    "            #SpectralNorm(nn.Conv2d(4*n_feature_maps, 8*n_feature_maps, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "        self.output = nn.Sequential(\n",
    "            # 4x4\n",
    "            #SpectralNorm(nn.Conv2d(8*n_feature_maps, 1, 4, 1, 0, bias=False)),\n",
    "            SNConv2d(4*n_feature_maps, 1, 4, 1, 0, bias=False),\n",
    "            #1x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def  forward(self, x, matching=False):\n",
    "        x = self.main(x)\n",
    "        if matching :\n",
    "            return x\n",
    "        else:\n",
    "            output = self.output(x)\n",
    "            return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5118\n",
       " 0.5235\n",
       " 0.5206\n",
       " 0.5283\n",
       " 0.4850\n",
       " 0.5419\n",
       " 0.5254\n",
       " 0.5065\n",
       " 0.5349\n",
       " 0.5075\n",
       " 0.5220\n",
       " 0.5577\n",
       " 0.5152\n",
       " 0.5197\n",
       " 0.5225\n",
       " 0.5079\n",
       " 0.5287\n",
       " 0.4900\n",
       " 0.4662\n",
       " 0.4714\n",
       " 0.5294\n",
       " 0.5244\n",
       " 0.5328\n",
       " 0.5440\n",
       " 0.4695\n",
       " 0.5066\n",
       " 0.5182\n",
       " 0.5350\n",
       " 0.5342\n",
       " 0.4870\n",
       " 0.5090\n",
       " 0.5094\n",
       " 0.5284\n",
       " 0.4809\n",
       " 0.5127\n",
       " 0.5358\n",
       " 0.4933\n",
       " 0.5171\n",
       " 0.4968\n",
       " 0.5151\n",
       " 0.4966\n",
       " 0.4908\n",
       " 0.5026\n",
       " 0.4967\n",
       " 0.5212\n",
       " 0.4942\n",
       " 0.5043\n",
       " 0.5220\n",
       " 0.5163\n",
       " 0.4821\n",
       " 0.4984\n",
       " 0.5195\n",
       " 0.5125\n",
       " 0.5447\n",
       " 0.4877\n",
       " 0.5121\n",
       " 0.5081\n",
       " 0.5106\n",
       " 0.4983\n",
       " 0.5039\n",
       " 0.4664\n",
       " 0.5190\n",
       " 0.5098\n",
       " 0.5310\n",
       " 0.4957\n",
       " 0.4943\n",
       " 0.5067\n",
       " 0.4898\n",
       " 0.4827\n",
       " 0.5078\n",
       " 0.5510\n",
       " 0.5058\n",
       " 0.5580\n",
       " 0.5208\n",
       " 0.5280\n",
       " 0.5268\n",
       " 0.5063\n",
       " 0.5398\n",
       " 0.5408\n",
       " 0.4798\n",
       " 0.5112\n",
       " 0.4952\n",
       " 0.5269\n",
       " 0.5426\n",
       " 0.4855\n",
       " 0.4930\n",
       " 0.5483\n",
       " 0.5172\n",
       " 0.5268\n",
       " 0.4823\n",
       " 0.4901\n",
       " 0.5149\n",
       " 0.5021\n",
       " 0.4715\n",
       " 0.4949\n",
       " 0.5488\n",
       " 0.5213\n",
       " 0.5153\n",
       " 0.5243\n",
       " 0.4813\n",
       " 0.5305\n",
       " 0.5017\n",
       " 0.5077\n",
       " 0.5175\n",
       " 0.5428\n",
       " 0.4984\n",
       " 0.5136\n",
       " 0.5430\n",
       " 0.5166\n",
       " 0.4931\n",
       " 0.5554\n",
       " 0.5250\n",
       " 0.4942\n",
       " 0.5443\n",
       " 0.5336\n",
       " 0.4887\n",
       " 0.4896\n",
       " 0.5344\n",
       " 0.5009\n",
       " 0.4919\n",
       " 0.4976\n",
       " 0.5265\n",
       " 0.4974\n",
       " 0.5235\n",
       " 0.5046\n",
       " 0.5309\n",
       " 0.5006\n",
       " 0.4899\n",
       " 0.5517\n",
       " 0.5597\n",
       " 0.5117\n",
       " 0.5299\n",
       " 0.4580\n",
       " 0.5460\n",
       " 0.5023\n",
       " 0.5081\n",
       " 0.5198\n",
       " 0.5019\n",
       " 0.5514\n",
       " 0.4924\n",
       " 0.4660\n",
       " 0.4879\n",
       " 0.4890\n",
       " 0.5032\n",
       " 0.4710\n",
       " 0.5255\n",
       " 0.5442\n",
       " 0.5140\n",
       " 0.5180\n",
       " 0.5315\n",
       " 0.5050\n",
       " 0.5203\n",
       " 0.5308\n",
       " 0.4933\n",
       " 0.5361\n",
       " 0.5267\n",
       " 0.5049\n",
       " 0.5033\n",
       " 0.5174\n",
       " 0.4999\n",
       " 0.5266\n",
       " 0.4883\n",
       " 0.5111\n",
       " 0.4714\n",
       " 0.5294\n",
       " 0.5266\n",
       " 0.5422\n",
       " 0.4977\n",
       " 0.4869\n",
       " 0.5048\n",
       " 0.5204\n",
       " 0.5107\n",
       " 0.5374\n",
       " 0.5096\n",
       " 0.4822\n",
       " 0.4821\n",
       " 0.5609\n",
       " 0.5138\n",
       " 0.5456\n",
       " 0.5327\n",
       " 0.5110\n",
       " 0.4841\n",
       " 0.5106\n",
       " 0.5171\n",
       " 0.4961\n",
       " 0.4975\n",
       " 0.4658\n",
       " 0.5176\n",
       " 0.5463\n",
       " 0.4995\n",
       " 0.5091\n",
       " 0.5138\n",
       " 0.4964\n",
       " 0.5093\n",
       " 0.4799\n",
       " 0.5072\n",
       " 0.5266\n",
       " 0.4937\n",
       " 0.5441\n",
       " 0.4897\n",
       " 0.5287\n",
       " 0.4941\n",
       " 0.5466\n",
       " 0.5260\n",
       " 0.5363\n",
       " 0.5173\n",
       " 0.4863\n",
       " 0.5066\n",
       " 0.4892\n",
       " 0.5139\n",
       " 0.5134\n",
       " 0.5321\n",
       " 0.5293\n",
       " 0.5153\n",
       " 0.5001\n",
       " 0.5264\n",
       " 0.5238\n",
       " 0.5035\n",
       " 0.4860\n",
       " 0.5079\n",
       " 0.5301\n",
       " 0.4911\n",
       " 0.4888\n",
       " 0.5234\n",
       " 0.5167\n",
       " 0.5548\n",
       " 0.5051\n",
       " 0.5408\n",
       " 0.5050\n",
       " 0.5368\n",
       " 0.5147\n",
       " 0.5420\n",
       " 0.4956\n",
       " 0.5143\n",
       " 0.4735\n",
       " 0.5044\n",
       " 0.5116\n",
       " 0.4859\n",
       " 0.4737\n",
       " 0.4750\n",
       " 0.5217\n",
       " 0.4862\n",
       " 0.5031\n",
       " 0.5159\n",
       " 0.5042\n",
       " 0.5177\n",
       " 0.5191\n",
       " 0.4915\n",
       " 0.5208\n",
       " 0.4813\n",
       " 0.4999\n",
       " 0.5240\n",
       " 0.5143\n",
       " 0.5305\n",
       " 0.4715\n",
       " 0.5077\n",
       " 0.5151\n",
       " 0.5177\n",
       " 0.5060\n",
       " 0.5021\n",
       " 0.4795\n",
       " 0.4837\n",
       " 0.5083\n",
       " 0.5023\n",
       " 0.4883\n",
       " 0.5040\n",
       " 0.4967\n",
       " 0.5537\n",
       " 0.5109\n",
       " 0.5321\n",
       " 0.4978\n",
       " 0.4864\n",
       " 0.5090\n",
       " 0.4863\n",
       " 0.5035\n",
       " 0.5460\n",
       " 0.5366\n",
       " 0.5079\n",
       " 0.4900\n",
       " 0.5014\n",
       " 0.4639\n",
       " 0.5366\n",
       " 0.5137\n",
       " 0.5024\n",
       " 0.5305\n",
       " 0.5144\n",
       " 0.4735\n",
       " 0.5267\n",
       " 0.5303\n",
       " 0.4979\n",
       " 0.4944\n",
       " 0.5271\n",
       " 0.5030\n",
       " 0.4827\n",
       " 0.4897\n",
       " 0.5152\n",
       " 0.5066\n",
       " 0.5181\n",
       " 0.5019\n",
       " 0.5044\n",
       " 0.5067\n",
       " 0.5291\n",
       " 0.5637\n",
       " 0.5343\n",
       " 0.5307\n",
       " 0.5002\n",
       " 0.5176\n",
       " 0.5148\n",
       " 0.5058\n",
       " 0.5146\n",
       " 0.4794\n",
       " 0.5258\n",
       " 0.5015\n",
       " 0.4941\n",
       " 0.5074\n",
       " 0.5608\n",
       " 0.4808\n",
       " 0.5377\n",
       " 0.5361\n",
       " 0.5247\n",
       " 0.5232\n",
       " 0.5094\n",
       " 0.5045\n",
       " 0.5432\n",
       " 0.5180\n",
       " 0.5097\n",
       " 0.5023\n",
       " 0.5070\n",
       " 0.4920\n",
       " 0.5447\n",
       " 0.5288\n",
       " 0.5184\n",
       " 0.4926\n",
       " 0.5030\n",
       " 0.5242\n",
       " 0.5290\n",
       " 0.5158\n",
       " 0.4901\n",
       " 0.5345\n",
       " 0.4699\n",
       " 0.4783\n",
       " 0.5219\n",
       " 0.4815\n",
       " 0.5210\n",
       " 0.5089\n",
       " 0.5025\n",
       " 0.5003\n",
       " 0.5087\n",
       " 0.5125\n",
       " 0.5106\n",
       " 0.5186\n",
       " 0.5024\n",
       " 0.5055\n",
       " 0.5092\n",
       " 0.5453\n",
       " 0.5109\n",
       " 0.5067\n",
       " 0.5051\n",
       " 0.5090\n",
       " 0.5549\n",
       " 0.5303\n",
       " 0.5380\n",
       " 0.4869\n",
       " 0.5089\n",
       " 0.5141\n",
       " 0.5073\n",
       " 0.4990\n",
       " 0.4897\n",
       " 0.4636\n",
       " 0.4952\n",
       " 0.4953\n",
       " 0.4985\n",
       " 0.5095\n",
       " 0.5073\n",
       " 0.4945\n",
       " 0.4967\n",
       " 0.5391\n",
       " 0.5543\n",
       " 0.5507\n",
       " 0.5090\n",
       " 0.5154\n",
       " 0.4952\n",
       " 0.5157\n",
       " 0.4981\n",
       " 0.5034\n",
       " 0.4628\n",
       " 0.4984\n",
       " 0.4919\n",
       " 0.4826\n",
       " 0.4769\n",
       " 0.4670\n",
       " 0.5405\n",
       " 0.5065\n",
       " 0.5320\n",
       " 0.5529\n",
       " 0.5431\n",
       " 0.5219\n",
       " 0.5679\n",
       " 0.5386\n",
       " 0.5362\n",
       " 0.5279\n",
       " 0.4997\n",
       " 0.5329\n",
       " 0.5244\n",
       " 0.5087\n",
       " 0.5166\n",
       " 0.5097\n",
       " 0.5142\n",
       " 0.4606\n",
       " 0.4691\n",
       " 0.4882\n",
       " 0.4513\n",
       " 0.5312\n",
       " 0.5081\n",
       " 0.5141\n",
       " 0.5039\n",
       " 0.5208\n",
       " 0.4868\n",
       " 0.4935\n",
       " 0.4765\n",
       " 0.5338\n",
       " 0.4854\n",
       " 0.5681\n",
       " 0.5567\n",
       " 0.5331\n",
       " 0.5219\n",
       " 0.5281\n",
       " 0.5135\n",
       " 0.4866\n",
       " 0.5086\n",
       " 0.4875\n",
       " 0.5271\n",
       " 0.4888\n",
       " 0.5306\n",
       " 0.5289\n",
       " 0.4742\n",
       " 0.5086\n",
       " 0.5080\n",
       " 0.5140\n",
       " 0.4832\n",
       " 0.4922\n",
       " 0.4772\n",
       " 0.5280\n",
       " 0.5118\n",
       " 0.4491\n",
       " 0.4966\n",
       " 0.5278\n",
       " 0.5004\n",
       " 0.5688\n",
       " 0.4847\n",
       " 0.4815\n",
       " 0.5141\n",
       " 0.5267\n",
       " 0.5459\n",
       " 0.4998\n",
       " 0.5172\n",
       " 0.5240\n",
       " 0.5610\n",
       " 0.4648\n",
       " 0.5125\n",
       " 0.4735\n",
       " 0.5613\n",
       " 0.4879\n",
       " 0.4983\n",
       " 0.5192\n",
       " 0.5151\n",
       " 0.4517\n",
       " 0.5274\n",
       " 0.5202\n",
       " 0.4730\n",
       " 0.5324\n",
       " 0.5098\n",
       " 0.5187\n",
       " 0.5255\n",
       " 0.4897\n",
       " 0.5051\n",
       " 0.5371\n",
       " 0.5691\n",
       " 0.5047\n",
       " 0.5481\n",
       " 0.5059\n",
       " 0.5228\n",
       " 0.5098\n",
       " 0.5417\n",
       " 0.5226\n",
       " 0.4836\n",
       " 0.4741\n",
       " 0.4991\n",
       " 0.5259\n",
       " 0.4739\n",
       " 0.5831\n",
       " 0.5174\n",
       " 0.5467\n",
       " 0.5470\n",
       " 0.4950\n",
       " 0.5520\n",
       " 0.5098\n",
       " 0.5026\n",
       " 0.5275\n",
       " 0.4908\n",
       "[torch.FloatTensor of size 500]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = SNDiscriminator(64)\n",
    "t = Variable(torch.rand(20,3,64,64))\n",
    "D(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "\t[\n",
    "\t    transforms.ToTensor(),\n",
    "\t    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "\t])\n",
    "dataset = datasets.ImageFolder('paintings64/', transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SNDiscriminator(\n",
       "  (main): Sequential(\n",
       "    (0): SNConv2d (3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(0.2, inplace)\n",
       "    (2): SNConv2d (128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): LeakyReLU(0.2, inplace)\n",
       "    (4): SNConv2d (256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (5): LeakyReLU(0.2, inplace)\n",
       "    (6): LeakyReLU(0.2, inplace)\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): SNConv2d (512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zdim = 100\n",
    "n_feature_maps = 128\n",
    "n_epochs = 50\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv' or 'SNConv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "G = Generator(zdim, n_feature_maps)\n",
    "G.apply(weights_init)\n",
    "D = SNDiscriminator(n_feature_maps)\n",
    "D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "G_optimiser = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "D_optimiser = optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n",
      "kjd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-17:\n",
      "Process Process-18:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/francois/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-7780693b5ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mG_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mG_optimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-a85ca92d474e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m  \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    599\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    600\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv_transpose2d\u001b[0;34m(input, weight, bias, stride, padding, output_padding, groups, dilation)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_padding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1,n_epochs+1):\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "    for img, label in dataloader:\n",
    "        print('kjd')\n",
    "        x = Variable(img)\n",
    "\n",
    "        # D training, n_critic=1\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        D.zero_grad\n",
    "        D_real = D(x)\n",
    "\n",
    "        z = torch.FloatTensor(x.size(0), zdim, 1, 1).normal_()\n",
    "\n",
    "        z = Variable(z)\n",
    "        fake = G(z)\n",
    "        D_fake = D(fake.detach())        \n",
    "        D_err = torch.mean(D_real) - torch.mean(D_fake)\n",
    "        D_optimiser.step()\n",
    "\n",
    "        # G training\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = False # saves computation\n",
    "\n",
    "        z = torch.FloatTensor(batch_size, zdim, 1, 1).normal_()\n",
    "\n",
    "        z = Variable(z)\n",
    "        fake = G(z)\n",
    "        G_err = torch.mean(D(fake))\n",
    "        G_optimiser.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
