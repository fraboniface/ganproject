{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive growing of GANs prototype\n",
    "\n",
    "To add:\n",
    "- smooth transitions -> hard\n",
    "- pixel-wise feature normalisation in generator -> easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.nn.functional as F\n",
    "from torchvision  import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO IMPLEMENT\n",
    "# you can add these to a Sequential\n",
    "\n",
    "class MinibatchSDLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinibatchSDLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_batch_std = x.std(0).mean()\n",
    "        mean_batch_std = mean_batch_std.expand(x.size(0), 1, x.size(-1), x.size(-1))\n",
    "        return torch.cat([x, mean_batch_std], 1)\n",
    "    \n",
    "    \n",
    "class PixelWiseFeatureNormLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelWiseFeatureNormLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SpectralNormLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectralNormLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrowingGenerator(nn.Module):\n",
    "    def __init__(self, zdim=100, init_size=4, final_size=128, n_feature_maps=128):\n",
    "        super(GrowingGenerator, self).__init__()\n",
    "       \n",
    "        self.init_size = init_size\n",
    "        self.final_size = final_size\n",
    "        init_nfm = 8*n_feature_maps\n",
    "        \n",
    "        self.layers = [\n",
    "            #1x1\n",
    "            nn.ConvTranspose2d(zdim, init_nfm, 4, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #4x4\n",
    "            nn.Conv2d(init_nfm, init_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "            #4x4\n",
    "        ]\n",
    "        self.main = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.to_rgb = nn.Conv2d(init_nfm, 3, 1, 1, 0, bias=False)\n",
    "        self.current_size = init_size\n",
    "        self.current_nfm = init_nfm\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        x = self.to_rgb(x)\n",
    "        return F.tanh(x)\n",
    "\n",
    "    def grow(self):\n",
    "        if self.current_size == self.final_size:\n",
    "            print(\"Network can't grow more\")\n",
    "            return\n",
    "        \n",
    "        if self.current_size in [8,32]: # don't decrease everytime because otherwise it's too fast\n",
    "            future_nfm = self.current_nfm\n",
    "        else:\n",
    "            future_nfm = int(self.current_nfm / 2)\n",
    "            \n",
    "        block = [\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(self.current_nfm, future_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(future_nfm, future_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        self.layers += block\n",
    "        self.main = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.current_size *= 2\n",
    "        self.current_nfm = future_nfm\n",
    "        self.to_rgb = nn.Conv2d(self.current_nfm, 3, 1, 1, 0, bias=False)\n",
    "        \n",
    "        self.new_parameters = nn.Sequential(*block).parameters()\n",
    "        \n",
    "        \n",
    "class GrowingDiscriminator(nn.Module):\n",
    "    def __init__(self, init_size=4, final_size=128, n_feature_maps=128):\n",
    "        super(GrowingDiscriminator, self).__init__()\n",
    "        self.init_size = init_size\n",
    "        self.final_size = final_size\n",
    "        init_nfm = 8 * n_feature_maps\n",
    "        \n",
    "        self.from_rgb = nn.Sequential(\n",
    "            nn.Conv2d(3, init_nfm, 1, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layers = [\n",
    "            #4x4\n",
    "            nn.Conv2d(init_nfm, init_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #4x4\n",
    "            #nn.Conv2d(init_nfm, init_nfm, 4, 1, 0, bias=False),\n",
    "            nn.Conv2d(init_nfm, 1, 4, 1, 0, bias=False),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #1x1\n",
    "            #nn.Conv2d(init_nfm, 1, 1, 1, 0, bias=False) # equivalent to fully connected\n",
    "            #nn.Sigmoid()\n",
    "        ]\n",
    "        self.main = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.current_size = init_size\n",
    "        self.current_nfm = init_nfm\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.size(3) != self.current_size:\n",
    "            print(\"input is of the wrong size (should be {})\".format(self.current_size))\n",
    "            return\n",
    "        \n",
    "        x = self.from_rgb(x)\n",
    "        output = self.main(x)\n",
    "        return output.view(-1,1).squeeze()\n",
    "    \n",
    "    def grow(self):\n",
    "        if self.current_size == self.final_size:\n",
    "            print(\"Network can't grow more\")\n",
    "            return\n",
    "        \n",
    "        if self.current_size in [8,32]:\n",
    "            future_nfm = self.current_nfm\n",
    "        else:\n",
    "            future_nfm = int(self.current_nfm / 2)\n",
    "        \n",
    "        # if first growing, we had minibatch std\n",
    "        if self.current_size == self.init_size:\n",
    "            self.layers = [MinibatchSDLayer()] + self.layers\n",
    "        \n",
    "        block = [\n",
    "            nn.Conv2d(future_nfm, future_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(future_nfm, self.current_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.AvgPool2d(2)\n",
    "        ]\n",
    "        self.layers = block + self.layers\n",
    "        self.main = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.current_size *= 2\n",
    "        self.current_nfm = future_nfm\n",
    "        self.from_rgb = nn.Conv2d(3, self.current_nfm, 1, 1, 0, bias=False)\n",
    "        \n",
    "        self.new_parameters = nn.Sequential(*block).parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "\t[\n",
    "\t    transforms.ToTensor(),\n",
    "\t    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "\t])\n",
    "dataset = datasets.ImageFolder('paintings64/', transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GrowingDiscriminator(\n",
       "  (from_rgb): Sequential(\n",
       "    (0): Conv2d (3, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(0.2, inplace)\n",
       "  )\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d (1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(0.2, inplace)\n",
       "    (2): Conv2d (1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zdim = 100\n",
    "n_feature_maps = 128\n",
    "init_size = 4\n",
    "final_size = 64\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        \n",
    "G = GrowingGenerator(zdim, init_size, final_size, n_feature_maps)\n",
    "G.apply(weights_init)\n",
    "D = GrowingDiscriminator(init_size, final_size, n_feature_maps)\n",
    "D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "beta1 = 0\n",
    "beta2 = 0.99\n",
    "G_optimiser = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "D_optimiser = optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_penalty(real, fake, D, gamma=1):\n",
    "    batch_size = real.size(0)\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    alpha = Variable(alpha.expand_as(real))\n",
    "    interpolation = alpha*real + (1-alpha)*fake # everything is a Variable so interpolation should be one too\n",
    "    D_itp = D(interpolation)\n",
    "    gradients = grad(outputs=D_itp, inputs=interpolation, grad_outputs=torch.ones(D_itp.size()),\n",
    "                                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    GP = ((gradients.norm(2, dim=1) - gamma)**2 / gamma**2).mean()\n",
    "    return GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.5407  0.4825  0.5390\n",
       " 0.4180  0.5692  0.4473\n",
       " 0.6007  0.4951  0.4712\n",
       " 0.5359  0.6343  0.4671\n",
       " 0.5979  0.5003  0.4840\n",
       " 0.4008  0.5005  0.5363\n",
       " 0.4923  0.4290  0.3370\n",
       " 0.3789  0.4962  0.4651\n",
       " 0.4435  0.6083  0.4501\n",
       " 0.6282  0.5788  0.4825\n",
       " 0.4221  0.4190  0.4181\n",
       " 0.5917  0.4317  0.5368\n",
       " 0.4120  0.3518  0.5131\n",
       " 0.4476  0.4498  0.6718\n",
       " 0.3705  0.5237  0.5341\n",
       " 0.4924  0.4210  0.5256\n",
       " 0.4675  0.4763  0.5332\n",
       " 0.5430  0.4773  0.4204\n",
       " 0.5775  0.4399  0.5060\n",
       " 0.5643  0.5750  0.5323\n",
       " 0.5221  0.4944  0.5235\n",
       " 0.5078  0.4369  0.4284\n",
       " 0.4526  0.4259  0.5561\n",
       " 0.4584  0.5642  0.4732\n",
       " 0.5480  0.4675  0.3675\n",
       " 0.5367  0.6425  0.3746\n",
       " 0.4588  0.4288  0.4503\n",
       " 0.4756  0.4561  0.6355\n",
       " 0.5365  0.4252  0.4482\n",
       "[torch.FloatTensor of size 29x3]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "lambda_ = 10\n",
    "gamma = 750\n",
    "epsilon_drift = 1e-3\n",
    "examples_seen = 0\n",
    "current_size = 4\n",
    "for epoch in range(n_epochs):\n",
    "    for img, label in dataloader:\n",
    "        x = Variable(img)\n",
    "        if x.size(-1) > current_size:\n",
    "            ratio = int(x.size(0)/current_size)\n",
    "            x = F.avg_pool2d(x, ratio)\n",
    "        \n",
    "        # D training, n_critic=1\n",
    "        for p in D.parameters:\n",
    "            p.requires_grad = True\n",
    "            \n",
    "        D.zero_grad\n",
    "        D_real = D(x)\n",
    "        \n",
    "        z = torch.FloatTensor(batch_size, zdim, 1, 1).normal_()\n",
    "        z = Variable(z)\n",
    "        fake = G(z)\n",
    "        D_fake = D(fake.detach())\n",
    "        \n",
    "        GP = get_gradient_penalty(x, fake, D, gamma)\n",
    "        \n",
    "        D_err = torch.mean(D_real) - torch.mean(D_fake) + lambda_*GP + epsilon_drift*torch.mean(D_real**2)\n",
    "        D_optimiser.step()\n",
    "        \n",
    "        # G training\n",
    "        for p in D.parameters:\n",
    "            p.requires_grad = False # saves computation\n",
    "            \n",
    "        z = torch.FloatTensor(batch_size, zdim, 1, 1).normal_()\n",
    "        z = Variable(z)\n",
    "        fake = G(z)\n",
    "        G_err = torch.mean(D(fake))\n",
    "        G_optimiser.step()\n",
    "        \n",
    "        examples_seen += img.size(0)\n",
    "    \n",
    "    # we grow every 100K images. 600Kin the paper, plus transitions, we'll see\n",
    "    if examples_seen % 1e5 == 0:\n",
    "        examples_seen = 0\n",
    "        current_size *= 2\n",
    "        G.grow()\n",
    "        G_optimiser.add_param_group({'params': G.new_parameters})\n",
    "        D.grow()\n",
    "        D_optimiser.add_param_group({'params': D.new_parameters})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = GrowingGenerator(zdim=100, init_size=4, final_size=64, n_feature_maps=128)\n",
    "G.load_state_dict(torch.load('results/saved_data/paintings64_PG_GAN_generator', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.current_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
