{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive growing of GANs prototype\n",
    "\n",
    "- smooth transitions\n",
    "- minibatch STD\n",
    "- pixel-wise feature vector normalisation (not in the networks)\n",
    "- spectral normalisation\n",
    "- He initialisation\n",
    "- chrome tire rim\n",
    "- 7.1 Dolby Surround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import conv\n",
    "from torch.nn.modules.utils import _pair\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.nn.functional as F\n",
    "from torchvision  import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchSDLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MinibatchSDLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_batch_std = x.std(0).mean()\n",
    "        mean_batch_std = mean_batch_std.expand(x.size(0), 1, x.size(-1), x.size(-1))\n",
    "        return torch.cat([x, mean_batch_std], 1)\n",
    "    \n",
    "    \n",
    "class PixelWiseFeatureNormLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelWiseFeatureNormLayer, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x / x.norm(2,1)\n",
    "    \n",
    "    \n",
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "def max_singular_value(W, u=None, Ip=1):\n",
    "    \"\"\"\n",
    "    power iteration for weight parameter\n",
    "    \"\"\"\n",
    "    if u is None:\n",
    "        u = torch.FloatTensor(1, W.size(0)).normal_()\n",
    "        \n",
    "    _u = u\n",
    "    for _ in range(Ip):\n",
    "        _v = l2normalize(torch.matmul(_u, W), eps=1e-12)\n",
    "        _u = l2normalize(torch.matmul(_v, torch.transpose(W, 0, 1)), eps=1e-12)\n",
    "        \n",
    "    sigma = torch.sum(F.linear(_u, torch.transpose(W, 0, 1)) * _v)\n",
    "    return sigma, _u\n",
    "    \n",
    "    \n",
    "class SNConv2d(conv._ConvNd):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(SNConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, False, _pair(0), groups, bias)\n",
    "        self.u = nn.Parameter(torch.Tensor(1, out_channels).normal_(), requires_grad=False)\n",
    "        #self.u = torch.Tensor(1, out_channels).normal_()\n",
    "\n",
    "    @property\n",
    "    def W_(self):\n",
    "        w_mat = self.weight.view(self.weight.size(0), -1).data\n",
    "        sigma, _u = max_singular_value(w_mat, self.u.data)\n",
    "        self.u.data = _u\n",
    "        return self.weight / sigma\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.conv2d(input, self.W_, self.bias, self.stride, self.padding, self.dilation, self.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrowingGenerator(nn.Module):\n",
    "    def __init__(self, zdim=100, init_size=4, final_size=128, n_feature_maps=128):\n",
    "        super(GrowingGenerator, self).__init__()\n",
    "       \n",
    "        self.init_size = init_size\n",
    "        self.final_size = final_size\n",
    "        init_nfm = 8*n_feature_maps\n",
    "        \n",
    "        self.layers = [\n",
    "            #1x1\n",
    "            nn.ConvTranspose2d(zdim, init_nfm, 4, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #4x4\n",
    "            nn.Conv2d(init_nfm, init_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "            #4x4\n",
    "        ]\n",
    "        self.main = nn.Sequential(*self.layers)\n",
    "        self.old = self.main\n",
    "        \n",
    "        self.to_rgb = nn.Conv2d(init_nfm, 3, 1, 1, 0, bias=False)\n",
    "        self.old_rgb = self.to_rgb\n",
    "        self.current_size = init_size\n",
    "        self.current_nfm = init_nfm\n",
    "        \n",
    "        self.transitioning = False\n",
    "        \n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self._alpha\n",
    "        \n",
    "    @alpha.setter\n",
    "    def alpha(self, v):\n",
    "        if v > 1:\n",
    "            self.transitioning = False\n",
    "        self._alpha = v\n",
    "                \n",
    "    def forward(self, x):\n",
    "        if self.transitioning:\n",
    "            new = self.main(x)\n",
    "            new = self.to_rgb(new)\n",
    "            old = F.upsample(self.old(x), scale_factor=2)\n",
    "            old = self.old_rgb(old)\n",
    "            x = self.alpha*new + (1-self.alpha)*old\n",
    "        \n",
    "        else:   \n",
    "            x = self.main(x)\n",
    "            x = self.to_rgb(x)\n",
    "            \n",
    "        return F.tanh(x)\n",
    "    \n",
    "    def grow(self):\n",
    "        if self.current_size == self.final_size:\n",
    "            print(\"Network can't grow more\")\n",
    "            return\n",
    "        \n",
    "        self.transitioning = True\n",
    "        self._alpha = 0\n",
    "        \n",
    "        if self.current_size in [8,32]: # don't decrease everytime because otherwise it's too fast\n",
    "            future_nfm = self.current_nfm\n",
    "        else:\n",
    "            future_nfm = int(self.current_nfm / 2)\n",
    "            \n",
    "        self.old = self.main\n",
    "        self.old_rgb = self.to_rgb\n",
    "            \n",
    "        block = [\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(self.current_nfm, future_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(future_nfm, future_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        self.layers += block\n",
    "        self.main = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.current_size *= 2\n",
    "        self.current_nfm = future_nfm\n",
    "        self.to_rgb = nn.Conv2d(self.current_nfm, 3, 1, 1, 0, bias=False)\n",
    "        \n",
    "        self.new_parameters = nn.Sequential(*block).parameters()\n",
    "        \n",
    "        \n",
    "class GrowingDiscriminator(nn.Module):\n",
    "    def __init__(self, init_size=4, final_size=128, n_feature_maps=128):\n",
    "        super(GrowingDiscriminator, self).__init__()\n",
    "        self.init_size = init_size\n",
    "        self.final_size = final_size\n",
    "        init_nfm = 8 * n_feature_maps\n",
    "        \n",
    "        self.from_rgb = nn.Sequential(\n",
    "            SNConv2d(3, init_nfm, 1, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layers = [\n",
    "            MinibatchSDLayer(),\n",
    "            #4x4\n",
    "            SNConv2d(init_nfm+1, init_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            #4x4\n",
    "            #nn.Conv2d(init_nfm, init_nfm, 4, 1, 0, bias=False),\n",
    "            SNConv2d(init_nfm, 1, 4, 1, 0, bias=False),\n",
    "            #nn.LeakyReLU(0.2, inplace=True),\n",
    "            #1x1\n",
    "            #nn.Conv2d(init_nfm, 1, 1, 1, 0, bias=False) # equivalent to fully connected\n",
    "            #nn.Sigmoid()\n",
    "        ]\n",
    "        self.main = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.current_size = init_size\n",
    "        self.current_nfm = init_nfm\n",
    "        \n",
    "        self.transitioning = False\n",
    "        \n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self._alpha\n",
    "        \n",
    "    @alpha.setter\n",
    "    def alpha(self, v):\n",
    "        if v > 1:\n",
    "            self.transitioning = False\n",
    "        self._alpha = v\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.size(3) != self.current_size:\n",
    "            print(\"input is of the wrong size (should be {})\".format(self.current_size))\n",
    "            return\n",
    "        \n",
    "        if self.transitioning:\n",
    "            new = self.from_rgb(x)\n",
    "            new = self.new(new)\n",
    "            old = F.avg_pool2d(x, 2)\n",
    "            old = self.old_rgb(old)\n",
    "            x = self.alpha*new + (1-self.alpha)*old\n",
    "            output = self.old(x)\n",
    "            \n",
    "        else:\n",
    "            x = self.from_rgb(x)\n",
    "            output = self.main(x)\n",
    "            \n",
    "        return output.view(-1,1).squeeze()\n",
    "    \n",
    "    def grow(self):\n",
    "        if self.current_size == self.final_size:\n",
    "            print(\"Network can't grow more\")\n",
    "            return\n",
    "        \n",
    "        self.transitioning = True\n",
    "        self.alpha = 0\n",
    "        \n",
    "        if self.current_size in [8,32]:\n",
    "            future_nfm = self.current_nfm\n",
    "        else:\n",
    "            future_nfm = int(self.current_nfm / 2)\n",
    "            \n",
    "        self.old = self.main\n",
    "        self.old_rgb = self.from_rgb\n",
    "        \n",
    "        block = [\n",
    "            SNConv2d(future_nfm, future_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            SNConv2d(future_nfm, self.current_nfm, 3, 1, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.AvgPool2d(2)\n",
    "        ]\n",
    "        self.new = nn.Sequential(*block)\n",
    "        self.layers = block + self.layers\n",
    "        self.main = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.current_size *= 2\n",
    "        self.current_nfm = future_nfm\n",
    "        self.from_rgb = SNConv2d(3, self.current_nfm, 1, 1, 0, bias=False)\n",
    "        \n",
    "        self.new_parameters = nn.Sequential(*block).parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "\t[\n",
    "        transforms.Resize(4),\n",
    "\t    transforms.ToTensor(),\n",
    "\t    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "\t])\n",
    "dataset = datasets.ImageFolder('paintings64/', transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GrowingDiscriminator(\n",
       "  (from_rgb): Sequential(\n",
       "    (0): SNConv2d (3, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(0.2, inplace)\n",
       "  )\n",
       "  (main): Sequential(\n",
       "    (0): MinibatchSDLayer(\n",
       "    )\n",
       "    (1): SNConv2d (1025, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): LeakyReLU(0.2, inplace)\n",
       "    (3): SNConv2d (1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zdim = 100\n",
    "n_feature_maps = 128\n",
    "init_size = 4\n",
    "final_size = 64\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.kaiming_uniform(m.weight.data)\n",
    "        \n",
    "G = GrowingGenerator(zdim, init_size, final_size, n_feature_maps)\n",
    "G.apply(weights_init)\n",
    "D = GrowingDiscriminator(init_size, final_size, n_feature_maps)\n",
    "D.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "beta1 = 0\n",
    "beta2 = 0.99\n",
    "G_optimiser = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "D_optimiser = optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), lr=lr, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_penalty(real, fake, D, gamma=1, gpu=True):\n",
    "    batch_size = real.size(0)\n",
    "    alpha = torch.rand(batch_size,1,1,1)\n",
    "    alpha = Variable(alpha.expand_as(real))\n",
    "    if gpu:\n",
    "        alpha = alpha.cuda()\n",
    "\n",
    "    interpolation = alpha * real + (1-alpha) * fake # everything is a Variable so interpolation should be one too\n",
    "    D_itp = D(interpolation)\n",
    "    if gpu:\n",
    "        gradients = grad(outputs=D_itp, inputs=interpolation, grad_outputs=torch.ones(D_itp.size()).cuda(), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    else:\n",
    "        gradients = grad(outputs=D_itp, inputs=interpolation, grad_outputs=torch.ones(D_itp.size()), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    GP = ((gradients.norm(2, dim=1) - gamma)**2 / gamma**2).mean()\n",
    "    return GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "64\n",
      "128\n",
      "192\n",
      "0\n",
      "input is of the wrong size (should be 8)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torch.mean received an invalid combination of arguments - got (NoneType), but expected one of:\n * (torch.FloatTensor source)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mNoneType\u001b[0m)\n * (torch.FloatTensor source, int dim)\n * (torch.FloatTensor source, int dim, bool keepdim)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6d0abbf72a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mD_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mD_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_real\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mD_optimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.mean received an invalid combination of arguments - got (NoneType), but expected one of:\n * (torch.FloatTensor source)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mNoneType\u001b[0m)\n * (torch.FloatTensor source, int dim)\n * (torch.FloatTensor source, int dim, bool keepdim)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "lambda_ = 10\n",
    "gamma = 750\n",
    "epsilon_drift = 1e-3\n",
    "examples_seen = 0\n",
    "current_size = 4\n",
    "for epoch in range(n_epochs):\n",
    "    for img, label in dataloader:\n",
    "        print(examples_seen)\n",
    "        x = Variable(img)\n",
    "        #if x.size(-1) > current_size:\n",
    "        #    ratio = int(x.size(0)/current_size)\n",
    "        #    x = F.avg_pool2d(x, ratio)\n",
    "        \n",
    "        # D training, n_critic=1\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = True\n",
    "            \n",
    "        D.zero_grad\n",
    "        D_real = D(x)\n",
    "        \n",
    "        z = torch.FloatTensor(batch_size, zdim, 1, 1).normal_()\n",
    "        z = Variable(z)\n",
    "        fake = G(z)\n",
    "        D_fake = D(fake.detach())\n",
    "                \n",
    "        D_err = torch.mean(D_real) - torch.mean(D_fake)\n",
    "        D_optimiser.step()\n",
    "        \n",
    "        # G training\n",
    "        for p in D.parameters():\n",
    "            p.requires_grad = False # saves computation\n",
    "            \n",
    "        z = torch.FloatTensor(batch_size, zdim, 1, 1).normal_()\n",
    "        z = Variable(z)\n",
    "        fake = G(z)\n",
    "        G_err = torch.mean(D(fake))\n",
    "        G_optimiser.step()\n",
    "        \n",
    "        examples_seen += img.size(0)\n",
    "        \n",
    "        if G.transitioning:\n",
    "            G.alpha += 1e-3\n",
    "            D.alpha += 1e-3\n",
    "            print(G.alpha, D.alpha)\n",
    "        \n",
    "        \n",
    "    # we grow every 100K images. 600Kin the paper, plus transitions, we'll see\n",
    "        if examples_seen > 200:\n",
    "            examples_seen = 0\n",
    "            current_size *= 2\n",
    "            dataset.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(current_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "            ])\n",
    "            G.grow()\n",
    "            G_optimiser.add_param_group({'params': G.new_parameters})\n",
    "            D.grow()\n",
    "            D_optimiser.add_param_group({'params': filter(lambda p: p.requires_grad, D.new_parameters)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'torch.FloatTensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8ea7c123c33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \"\"\"\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Got inappropriate size arg: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'torch.FloatTensor'>"
     ]
    }
   ],
   "source": [
    "dataset.transform.transforms[0](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.75"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30000/64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
